#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub CVE Monitor - å¢å¼ºç‰ˆæ¯æ—¥æ•°æ®ç”Ÿæˆè„šæœ¬

åŠŸèƒ½:
1. ä»README.mdè§£æCVEæ•°æ®
2. æŒ‰æ—¥æœŸåˆ†ç»„ç”Ÿæˆå®Œæ•´çš„æ¯æ—¥JSONæ–‡ä»¶
3. å¡«è¡¥ç¼ºå¤±æ—¥æœŸçš„ç©ºJSONæ–‡ä»¶
4. ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æ±‡æ€»
5. ä¿®å¤å¢é•¿ç‡è®¡ç®—é—®é¢˜
"""

import os
import re
import json
import argparse
from datetime import datetime, date, timedelta
from collections import defaultdict

def parse_readme(readme_path):
    """è§£æREADME.mdæ–‡ä»¶ï¼Œæå–CVEæ•°æ®"""
    try:
        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"âŒ READMEæ–‡ä»¶æœªæ‰¾åˆ°: {readme_path}")
        return []
    
    # æŸ¥æ‰¾è¡¨æ ¼æ•°æ®å¼€å§‹ä½ç½®
    table_start = content.find('| CVE | ç›¸å…³ä»“åº“')
    if table_start == -1:
        print("âŒ æœªæ‰¾åˆ°CVEè¡¨æ ¼æ•°æ®")
        return []
    
    # æå–è¡¨æ ¼éƒ¨åˆ†
    table_section = content[table_start:]
    lines = table_section.split('\n')[2:]  # è·³è¿‡è¡¨å¤´
    
    cve_data = []
    print(f"ğŸ“‹ å¼€å§‹è§£æCVEæ•°æ®...")
    
    for line_num, line in enumerate(lines, 1):
        line = line.strip()
        if not line or not line.startswith('|'):
            continue
            
        # åˆ†å‰²è¡¨æ ¼åˆ—
        cols = [col.strip() for col in line.split('|') if col.strip()]
        if len(cols) < 4:
            continue
            
        try:
            # æå–CVE ID - æ”¯æŒå¤šç§æ ¼å¼
            cve_match = re.search(r'CVE[_-]?\d{4}[_-]?\d{4,7}', cols[0])
            if not cve_match:
                continue
            cve_id = cve_match.group().replace('_', '-')
            
            # æå–ä»“åº“ä¿¡æ¯
            repo_info = cols[1]
            
            # æå–æè¿°
            description = cols[2]
            
            # æå–æ—¥æœŸ - æ”¯æŒå¤šç§æ ¼å¼
            date_str = cols[3].strip()
            date_patterns = [
                r'(\d{4}-\d{2}-\d{2})T\d{2}:\d{2}:\d{2}Z',  # ISOæ ¼å¼
                r'(\d{4}-\d{2}-\d{2})',  # ç®€å•æ—¥æœŸæ ¼å¼
                r'(\d{4})/(\d{2})/(\d{2})',  # æ–œæ æ ¼å¼
            ]
            
            parsed_date = None
            for pattern in date_patterns:
                date_match = re.search(pattern, date_str)
                if date_match:
                    try:
                        if '/' in pattern:
                            # å¤„ç†æ–œæ æ ¼å¼
                            year, month, day = date_match.groups()
                            parsed_date = date(int(year), int(month), int(day))
                        else:
                            parsed_date = datetime.strptime(date_match.group(1), '%Y-%m-%d').date()
                        break
                    except ValueError:
                        continue
            
            if not parsed_date:
                print(f"âš ï¸  ç¬¬{line_num}è¡Œæ—¥æœŸæ ¼å¼æ— æ³•è§£æ: {date_str}")
                continue
            
            cve_data.append({
                'cve_id': cve_id,
                'repo_info': repo_info,
                'description': description,
                'date': parsed_date.isoformat(),
                'raw_date': date_str
            })
            
            if line_num % 1000 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {line_num} è¡Œï¼Œæå–åˆ° {len(cve_data)} ä¸ªCVE")
                
        except Exception as e:
            print(f"âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {e}")
            continue
    
    print(f"âœ… è§£æå®Œæˆï¼æ€»è®¡æå–åˆ° {len(cve_data)} ä¸ªCVE")
    return cve_data

def group_by_date(cve_data):
    """æŒ‰æ—¥æœŸåˆ†ç»„CVEæ•°æ®"""
    daily_data = defaultdict(list)
    
    for cve in cve_data:
        date_key = cve['date']
        daily_data[date_key].append(cve)
    
    # è½¬æ¢ä¸ºæ™®é€šå­—å…¸å¹¶æ’åº
    sorted_daily = dict(sorted(daily_data.items()))
    
    print(f"ğŸ“… æ•°æ®åˆ†ç»„å®Œæˆ:")
    print(f"   - æ—¥æœŸèŒƒå›´: {min(sorted_daily.keys())} åˆ° {max(sorted_daily.keys())}")
    print(f"   - æ€»å¤©æ•°: {len(sorted_daily)} å¤©")
    
    return sorted_daily

def fill_missing_dates(daily_data, start_date=None, end_date=None):
    """å¡«è¡¥ç¼ºå¤±çš„æ—¥æœŸï¼Œç”Ÿæˆç©ºçš„JSONæ–‡ä»¶"""
    if not daily_data:
        return daily_data
    
    # ç¡®å®šæ—¥æœŸèŒƒå›´
    existing_dates = list(daily_data.keys())
    if start_date is None:
        start_date = datetime.strptime(min(existing_dates), '%Y-%m-%d').date()
    if end_date is None:
        end_date = datetime.strptime(max(existing_dates), '%Y-%m-%d').date()
    
    print(f"ğŸ” å¡«è¡¥ç¼ºå¤±æ—¥æœŸ: {start_date} åˆ° {end_date}")
    
    filled_data = {}
    current_date = start_date
    added_count = 0
    
    while current_date <= end_date:
        date_str = current_date.isoformat()
        
        if date_str in daily_data:
            filled_data[date_str] = daily_data[date_str]
        else:
            # åˆ›å»ºç©ºçš„æ—¥æœŸæ•°æ®
            filled_data[date_str] = []
            added_count += 1
        
        current_date += timedelta(days=1)
    
    print(f"âœ… å¡«è¡¥äº† {added_count} ä¸ªç¼ºå¤±æ—¥æœŸ")
    return filled_data

def generate_json_files(daily_data, output_dir):
    """ç”Ÿæˆæ¯æ—¥JSONæ–‡ä»¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    generated_files = []
    
    for date_str, cves in daily_data.items():
        # æ–‡ä»¶åæ ¼å¼: YYYY-MM-DD.json
        filename = f"{date_str}.json"
        filepath = os.path.join(output_dir, filename)
        
        # å‡†å¤‡JSONæ•°æ®
        json_data = {
            'date': date_str,
            'count': len(cves),
            'cves': cves,
            'generated_at': datetime.now().isoformat(),
            'metadata': {
                'total_cves': len(cves),
                'date_range': date_str,
                'source': 'README.md',
                'script_version': '2.0'
            }
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            generated_files.append({
                'file': filename,
                'date': date_str,
                'count': len(cves),
                'path': filepath
            })
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    return generated_files

def calculate_growth_stats(generated_files):
    """è®¡ç®—å¢é•¿ç»Ÿè®¡ä¿¡æ¯"""
    if len(generated_files) < 2:
        return []
    
    # æŒ‰æ—¥æœŸæ’åº
    sorted_files = sorted(generated_files, key=lambda x: x['date'])
    growth_stats = []
    cumulative_total = 0
    
    for i, file_data in enumerate(sorted_files):
        cumulative_total += file_data['count']
        
        # è®¡ç®—å¢é•¿ç‡
        if i > 0:
            prev_count = sorted_files[i-1]['count']
            if prev_count > 0:
                growth_rate = ((file_data['count'] - prev_count) / prev_count) * 100
            else:
                growth_rate = 0 if file_data['count'] == 0 else 100
        else:
            growth_rate = 0
        
        growth_stats.append({
            'date': file_data['date'],
            'daily_count': file_data['count'],
            'cumulative_total': cumulative_total,
            'growth_rate': round(growth_rate, 2)
        })
    
    return growth_stats

def generate_summary(generated_files, output_dir):
    """ç”Ÿæˆè¯¦ç»†çš„æ±‡æ€»ä¿¡æ¯"""
    # è®¡ç®—å¢é•¿ç»Ÿè®¡
    growth_stats = calculate_growth_stats(generated_files)
    
    summary = {
        'generated_at': datetime.now().isoformat(),
        'script_version': '2.0',
        'total_files': len(generated_files),
        'total_cves': sum(f['count'] for f in generated_files),
        'date_range': {
            'start': min(f['date'] for f in generated_files) if generated_files else None,
            'end': max(f['date'] for f in generated_files) if generated_files else None
        },
        'statistics': {
            'avg_daily_cves': round(sum(f['count'] for f in generated_files) / len(generated_files), 2) if generated_files else 0,
            'max_daily_cves': max(f['count'] for f in generated_files) if generated_files else 0,
            'min_daily_cves': min(f['count'] for f in generated_files) if generated_files else 0,
            'empty_days': len([f for f in generated_files if f['count'] == 0]),
            'active_days': len([f for f in generated_files if f['count'] > 0])
        },
        'growth_analysis': growth_stats,
        'recent_7_days': growth_stats[-7:] if len(growth_stats) >= 7 else growth_stats,
        'recent_30_days': growth_stats[-30:] if len(growth_stats) >= 30 else growth_stats,
        'daily_stats': generated_files
    }
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_path = os.path.join(output_dir, 'daily_summary.json')
    try:
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š æ±‡æ€»æ–‡ä»¶å·²ç”Ÿæˆ: {summary_path}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")
    
    return summary

def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ - ä½¿ç”¨å°å†™çš„dataç›®å½•ä¿æŒä¸€è‡´æ€§
    default_readme = os.path.join(project_root, 'docs', 'README.md')
    default_output = os.path.join(project_root, 'docs', 'data', 'daily')
    
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ¯æ—¥CVEæ•°æ®ç”Ÿæˆå™¨')
    parser.add_argument('--readme', '-r', 
                       default=default_readme,
                       help=f'README.mdæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_readme})')
    parser.add_argument('--output', '-o',
                       default=default_output,
                       help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {default_output})')
    parser.add_argument('--fill-gaps', '-f',
                       action='store_true',
                       help='å¡«è¡¥ç¼ºå¤±çš„æ—¥æœŸï¼ˆç”Ÿæˆç©ºçš„JSONæ–‡ä»¶ï¼‰')
    parser.add_argument('--start-date', '-s',
                       help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DDæ ¼å¼ï¼Œä»…åœ¨--fill-gapsæ—¶æœ‰æ•ˆ)')
    parser.add_argument('--end-date', '-e',
                       help='ç»“æŸæ—¥æœŸ (YYYY-MM-DDæ ¼å¼ï¼Œä»…åœ¨--fill-gapsæ—¶æœ‰æ•ˆ)')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    print("ğŸš€ GitHub CVE Monitor - å¢å¼ºç‰ˆæ¯æ—¥æ•°æ®ç”Ÿæˆå™¨ v2.0")
    print("=" * 60)
    
    # è§£æREADME
    print(f"ğŸ“– è¯»å–READMEæ–‡ä»¶: {args.readme}")
    cve_data = parse_readme(args.readme)
    
    if not cve_data:
        print("âŒ æœªæå–åˆ°CVEæ•°æ®ï¼Œè„šæœ¬é€€å‡º")
        return 1
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    print("\nğŸ“… æŒ‰æ—¥æœŸåˆ†ç»„æ•°æ®...")
    daily_data = group_by_date(cve_data)
    
    # å¡«è¡¥ç¼ºå¤±æ—¥æœŸï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.fill_gaps:
        print("\nğŸ”§ å¡«è¡¥ç¼ºå¤±æ—¥æœŸ...")
        start_date = None
        end_date = None
        
        if args.start_date:
            try:
                start_date = datetime.strptime(args.start_date, '%Y-%m-%d').date()
            except ValueError:
                print(f"âŒ å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯: {args.start_date}")
                return 1
        
        if args.end_date:
            try:
                end_date = datetime.strptime(args.end_date, '%Y-%m-%d').date()
            except ValueError:
                print(f"âŒ ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯: {args.end_date}")
                return 1
        
        daily_data = fill_missing_dates(daily_data, start_date, end_date)
    
    # ç”ŸæˆJSONæ–‡ä»¶
    print(f"\nğŸ“ ç”ŸæˆJSONæ–‡ä»¶åˆ°: {args.output}")
    generated_files = generate_json_files(daily_data, args.output)
    
    # ç”Ÿæˆæ±‡æ€»
    print("\nğŸ“Š ç”Ÿæˆè¯¦ç»†æ±‡æ€»ä¿¡æ¯...")
    summary = generate_summary(generated_files, args.output)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°: {len(generated_files)}")
    print(f"ğŸ“Š æ€»CVEæ•°é‡: {summary['total_cves']}")
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {summary['date_range']['start']} åˆ° {summary['date_range']['end']}")
    print(f"ğŸ“ˆ å¹³å‡æ¯æ—¥CVE: {summary['statistics']['avg_daily_cves']}")
    print(f"ğŸ¯ æ´»è·ƒå¤©æ•°: {summary['statistics']['active_days']}")
    print(f"ğŸ’¤ ç©ºç™½å¤©æ•°: {summary['statistics']['empty_days']}")
    
    if args.verbose and generated_files:
        print("\nğŸ“‹ æœ€è¿‘ç”Ÿæˆçš„æ–‡ä»¶:")
        for f in generated_files[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ªæ–‡ä»¶
            print(f"   - {f['file']}: {f['count']} ä¸ªCVE")
        
        print("\nğŸ“ˆ æœ€è¿‘7å¤©å¢é•¿è¶‹åŠ¿:")
        recent_growth = summary['recent_7_days']
        for day in recent_growth:
            growth_indicator = "ğŸ“ˆ" if day['growth_rate'] > 0 else "ğŸ“‰" if day['growth_rate'] < 0 else "â¡ï¸"
            print(f"   - {day['date']}: {day['daily_count']} ä¸ªCVE (å¢é•¿ç‡: {day['growth_rate']:+.1f}%) {growth_indicator}")
    
    return 0

if __name__ == '__main__':
    exit(main())