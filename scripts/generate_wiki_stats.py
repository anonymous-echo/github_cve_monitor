#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""GitHub CVE Monitor - Wikiç»Ÿè®¡æ•°æ®ç”Ÿæˆè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

import os
import json
import re
from datetime import datetime, timedelta
from collections import defaultdict
import argparse

def get_project_root():
    """
    è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œå¤„ç†åµŒå¥—ç›®å½•æƒ…å†µ
    è§£å†³GitHub Actionsç¯å¢ƒä¸­çš„ç›®å½•åµŒå¥—é—®é¢˜
    """
    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # é€çº§å‘ä¸ŠæŸ¥æ‰¾ï¼Œç›´åˆ°æ‰¾åˆ°é¡¹ç›®çš„æ ‡å¿—æ€§æ–‡ä»¶æˆ–ç›®å½•
    test_dir = current_dir
    max_depth = 5  # è®¾ç½®æœ€å¤§æŸ¥æ‰¾æ·±åº¦
    
    for _ in range(max_depth):
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¡¹ç›®æ ‡å¿—æ€§æ–‡ä»¶/ç›®å½•
        if os.path.exists(os.path.join(test_dir, 'main.py')) and \
           os.path.exists(os.path.join(test_dir, 'docs')) and \
           os.path.exists(os.path.join(test_dir, 'db')):
            return test_dir
        
        # å‘ä¸Šä¸€çº§ç›®å½•
        parent_dir = os.path.dirname(test_dir)
        if parent_dir == test_dir:  # åˆ°è¾¾æ–‡ä»¶ç³»ç»Ÿæ ¹ç›®å½•
            break
        test_dir = parent_dir
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆåŸå§‹é€»è¾‘ï¼‰
    return os.path.dirname(current_dir)

# APIè¯·æ±‚é…ç½®
CVE_API_URL = "https://cveawg.mitre.org/api/cve/{cve_id}"
API_TIMEOUT = 5  # ç§’
API_RETRY_MAX = 3
API_RETRY_DELAY = 2  # ç§’

def load_daily_summary(summary_path):
    """åŠ è½½æ¯æ—¥æ±‡æ€»æ•°æ®"""
    try:
        with open(summary_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ±‡æ€»æ–‡ä»¶æœªæ‰¾åˆ°: {summary_path}")
        return None
    except json.JSONDecodeError:
        print(f"âŒ æ±‡æ€»æ–‡ä»¶æ ¼å¼é”™è¯¯: {summary_path}")
        return None

def load_daily_files(daily_dir, days=30):
    """åŠ è½½æœ€è¿‘Nå¤©çš„æ¯æ—¥JSONæ–‡ä»¶ - ä¿®æ”¹ä¸ºè¯»å–ç›®å½•ä¸­æ‰€æœ‰JSONæ–‡ä»¶"""
    daily_files = []
    
    # é¦–å…ˆå°è¯•ç›´æ¥è¯»å–ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    try:
        for filename in os.listdir(daily_dir):
            if filename.endswith('.json') and filename != 'daily_summary.json':
                file_path = os.path.join(daily_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # ç¡®ä¿æ•°æ®åŒ…å«å¿…è¦çš„dateå­—æ®µ
                        if 'date' in data and 'cves' in data:
                            daily_files.append(data)
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
        
        print(f"ğŸ“ ç›´æ¥è¯»å–åˆ° {len(daily_files)} ä¸ªJSONæ–‡ä»¶")
        
        # å¦‚æœæ²¡æœ‰è¯»å–åˆ°æ–‡ä»¶ï¼Œå›é€€åˆ°åŸå§‹çš„åŸºäºæ—¥æœŸçš„æ–¹æ³•
        if not daily_files:
            print("âš ï¸  æœªæ‰¾åˆ°JSONæ–‡ä»¶ï¼Œå°è¯•åŸºäºå½“å‰æ—¥æœŸæŸ¥æ‰¾")
            today = datetime.now().date()
            
            for i in range(days):
                target_date = today - timedelta(days=i)
                date_str = target_date.isoformat()
                file_path = os.path.join(daily_dir, f"{date_str}.json")
                
                if os.path.exists(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            daily_files.append(data)
                    except Exception as e:
                        print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
    except Exception as e:
        print(f"âš ï¸  è¯»å–ç›®å½•å¤±è´¥: {e}")
    
    # æŒ‰æ—¥æœŸæ’åº
    return sorted(daily_files, key=lambda x: x.get('date', ''))

def get_cve_details(cve_id):
    """è·³è¿‡CVE APIè°ƒç”¨ä»¥æé«˜æ€§èƒ½"""
    return None

def analyze_cve_types(cve_data):
    """ç®€åŒ–çš„CVEç±»å‹åˆ†æï¼Œä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…"""
    # ç®€åŒ–çš„å…³é”®è¯åˆ†ç±»
    type_patterns = {
        'è¿œç¨‹ä»£ç æ‰§è¡Œ': [r'RCE', r'remote code execution', r'è¿œç¨‹ä»£ç æ‰§è¡Œ'],
        'æ³¨å…¥æ”»å‡»': [r'injection', r'æ³¨å…¥', r'SQL', r'XSS', r'CSRF'],
        'ææƒæ¼æ´': [r'privilege escalation', r'ææƒ', r'æƒé™æå‡'],
        'ä¿¡æ¯æ³„éœ²': [r'info disclosure', r'information disclosure', r'ä¿¡æ¯æ³„éœ²'],
        'è·¯å¾„éå†': [r'path traversal', r'traversal', r'ç›®å½•éå†'],
        'æ‹’ç»æœåŠ¡': [r'DoS', r'denial of service', r'æ‹’ç»æœåŠ¡'],
        'è®¤è¯ç»•è¿‡': [r'bypass', r'authentication bypass', r'ç»•è¿‡'],
        'ç¼“å†²åŒºæº¢å‡º': [r'buffer overflow', r'ç¼“å†²åŒºæº¢å‡º'],
        'å…¶ä»–': []  # é»˜è®¤åˆ†ç±»
    }
    
    type_count = defaultdict(int)
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            description = cve.get('description', '').lower()
            classified = False
            
            # ä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…
            for cve_type, patterns in type_patterns.items():
                if cve_type == 'å…¶ä»–':
                    continue
                for pattern in patterns:
                    if re.search(pattern, description, re.IGNORECASE):
                        type_count[cve_type] += 1
                        classified = True
                        break
                if classified:
                    break
            
            if not classified:
                type_count['å…¶ä»–'] += 1
    
    # è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨
    return sorted(type_count.items(), key=lambda x: x[1], reverse=True)

def analyze_poc_exp(cve_data):
    """ç®€åŒ–çš„POC/EXPåˆ†æ"""
    poc_keywords = ['poc', 'proof of concept', 'éªŒè¯']
    exp_keywords = ['exp', 'exploit', 'æ¼æ´åˆ©ç”¨', 'åˆ©ç”¨ä»£ç ']
    
    poc_count = 0
    exp_count = 0
    both_count = 0
    neither_count = 0
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            # ç®€åŒ–å†…å®¹æå–
            content = ' '.join([
                cve.get('repo_info', '').lower(),
                cve.get('description', '').lower(),
                cve.get('repo_name', '').lower()
            ])
            
            # ç®€å•åˆ¤æ–­
            has_poc = any(keyword in content for keyword in poc_keywords)
            has_exp = any(keyword in content for keyword in exp_keywords)
            
            # ç»Ÿè®¡ç»“æœ
            if has_poc and has_exp:
                both_count += 1
            elif has_poc:
                poc_count += 1
            elif has_exp:
                exp_count += 1
            else:
                neither_count += 1
    
    return {
        'ä»…POC': poc_count,
        'ä»…EXP': exp_count,
        'POC+EXP': both_count,
        'æ— POC/EXP': neither_count
    }

def calculate_trends(growth_stats, days=7):
    """è®¡ç®—è¶‹åŠ¿æ•°æ®"""
    if len(growth_stats) < days:
        return growth_stats
    
    # ç¡®ä¿è¿”å›çš„æ•°æ®ç»“æ„ä¸€è‡´ï¼Œå¤„ç†é”®åå¯èƒ½ä¸åŒ¹é…çš„æƒ…å†µ
    result = []
    for item in growth_stats[-days:]:
        # å¤„ç†å¯èƒ½çš„é”®åå·®å¼‚
        formatted_item = {
            'date': item.get('date', ''),
            'daily_count': item.get('daily_count', item.get('count', 0)),
            'cumulative_total': item.get('cumulative_total', item.get('cumulative', 0)),
            'growth_rate': item.get('growth_rate', 0)
        }
        result.append(formatted_item)
    
    return result

def analyze_vendor_product_stats(cve_data):
    """è·³è¿‡APIè°ƒç”¨ï¼Œè¿”å›ç©ºç»Ÿè®¡"""
    return {
        'vendors': {},
        'products': {},
        'vendor_product_pairs': {}
    }

def analyze_fingerprint_stats(cve_data):
    """ç®€åŒ–çš„æŠ€æœ¯æ ˆç»Ÿè®¡"""
    # ç®€åŒ–çš„æŠ€æœ¯æ ˆæ¨¡å¼
    simple_patterns = {
        'Java': [r'java', r'spring'],
        'Python': [r'python', r'django', r'flask'],
        'PHP': [r'php', r'thinkphp'],
        'JavaScript': [r'javascript', r'js', r'node', r'react', r'vue'],
        'Windows': [r'windows'],
        'Linux': [r'linux'],
        'å…¶ä»–': []
    }
    
    fingerprint_count = defaultdict(int)
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            content = ' '.join([
                cve.get('description', '').lower(),
                cve.get('repo_name', '').lower()
            ])
            
            matched = False
            for tech, patterns in simple_patterns.items():
                if tech == 'å…¶ä»–':
                    continue
                for pattern in patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        fingerprint_count[tech] += 1
                        matched = True
                        break
                if matched:
                    break
            
            if not matched:
                fingerprint_count['å…¶ä»–'] += 1
    
    return sorted(fingerprint_count.items(), key=lambda x: x[1], reverse=True)

def generate_stats_file(summary, daily_files, output_path):
    """ç®€åŒ–çš„ç»Ÿè®¡æ•°æ®ç”Ÿæˆ"""
    # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    cve_types = analyze_cve_types(daily_files)
    poc_exp_stats = analyze_poc_exp(daily_files)
    fingerprint_stats = analyze_fingerprint_stats(daily_files)
    
    # è®¡ç®—ç®€å•è¶‹åŠ¿ï¼ˆä»daily_filesç›´æ¥è®¡ç®—ï¼‰
    trends = []
    cumulative = 0
    for day in sorted(daily_files[-7:], key=lambda x: x.get('date', '')):
        daily_count = len(day.get('cves', []))
        cumulative += daily_count
        trends.append({
            'date': day.get('date', ''),
            'daily_count': daily_count,
            'cumulative_total': cumulative,
            'growth_rate': 0  # ç®€åŒ–è®¡ç®—
        })
    
    # å‡†å¤‡ç®€åŒ–çš„ç»Ÿè®¡æ•°æ®
    stats = {
        'generated_at': datetime.now().isoformat(),
        'summary': {
            'total_cves': summary.get('total_cves', 0),
            'date_range': summary.get('date_range', {})
        },
        'cve_types': dict(cve_types),
        'poc_exp_stats': poc_exp_stats,
        'fingerprint_stats': dict(fingerprint_stats[:10]),
        'trends': trends
    }
    
    # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶ï¼ˆæ— ç¼©è¿›ä»¥æé«˜é€Ÿåº¦ï¼‰
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False)
        return stats
    except Exception:
        return None

def generate_wiki_md(stats, output_md_path):
    """ç®€åŒ–çš„Markdownç”Ÿæˆ"""
    if not stats:
        return False
    
    # æå–åŸºæœ¬æ•°æ®
    summary = stats.get('summary', {})
    cve_types = stats.get('cve_types', {})
    poc_exp_stats = stats.get('poc_exp_stats', {})
    fingerprint_stats = stats.get('fingerprint_stats', {})
    trends = stats.get('trends', [])
    
    # ç”Ÿæˆç®€åŒ–çš„Markdownå†…å®¹
    md_content = f"""
# ç»Ÿè®¡æ•°æ®

## æ€»ä½“ç»Ÿè®¡
- **æ€»CVEè®°å½•æ•°**: {summary.get('total_cves', 0):,}
- **ç›‘æµ‹å‘¨æœŸ**: {summary.get('date_range', {}).get('start', 'æš‚æ— ')} è‡³ {summary.get('date_range', {}).get('end', 'æš‚æ— ')}
- **æ•°æ®æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d')}

## æ¯æ—¥å¢é•¿è¶‹åŠ¿

| æ—¥æœŸ | æ¯æ—¥æ–°å¢ | ç´¯è®¡æ€»æ•° |
|:---|:---|:---|
"""
    
    # æ·»åŠ ç®€å•è¶‹åŠ¿è¡¨æ ¼
    for trend in reversed(trends[-7:]):  # ä»…æ˜¾ç¤ºæœ€è¿‘7å¤©
        md_content += f"| {trend['date']} | {trend['daily_count']} | {trend['cumulative_total']:,} |\n"
    
    # æ·»åŠ ç®€åŒ–çš„CVEç±»å‹ç»Ÿè®¡
    if cve_types:
        md_content += "\n## CVEåˆ†ç±»ç»Ÿè®¡\n\n| ç±»å‹ | æ•°é‡ |\n|:---|:---|\n"
        for cve_type, count in list(cve_types.items())[:8]:  # ä»…æ˜¾ç¤ºå‰8ä¸ª
            md_content += f"| {cve_type} | {count:,} |\n"
    
    # ä¿å­˜Markdownæ–‡ä»¶
    try:
        output_dir = os.path.dirname(output_md_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        with open(output_md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        return True
    except Exception:
        return False

def main():
    # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    PROJECT_ROOT = get_project_root()
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
    default_summary = os.path.join(PROJECT_ROOT, 'docs', 'data', 'daily', 'daily_summary.json')
    default_daily_dir = os.path.join(PROJECT_ROOT, 'docs', 'data', 'daily')
    default_output_json = os.path.join(PROJECT_ROOT, 'docs', 'data', 'statistics', 'wiki_stats.json')
    default_output_md = os.path.join(PROJECT_ROOT, 'wiki_content', 'ç»Ÿè®¡æ•°æ®.md')
    
    parser = argparse.ArgumentParser(description='Wikiç»Ÿè®¡æ•°æ®ç”Ÿæˆå™¨')
    parser.add_argument('--summary', '-s', default=default_summary)
    parser.add_argument('--daily-dir', '-d', default=default_daily_dir)
    parser.add_argument('--output-json', '-j', default=default_output_json)
    parser.add_argument('--output-md', '-m', default=default_output_md)
    parser.add_argument('--days', '-n', type=int, default=14)  # å‡å°‘é»˜è®¤ç»Ÿè®¡å¤©æ•°
    
    args = parser.parse_args()
    
    # åŠ è½½æ±‡æ€»æ•°æ®
    summary = load_daily_summary(args.summary)
    if not summary:
        return 1
    
    # åŠ è½½æ¯æ—¥æ•°æ®ï¼ˆå‡å°‘åŠ è½½å¤©æ•°ï¼‰
    daily_files = load_daily_files(args.daily_dir, min(args.days, 14))
    
    # ç”Ÿæˆç»Ÿè®¡æ•°æ®
    stats = generate_stats_file(summary, daily_files, args.output_json)
    if not stats:
        return 1
    
    # ç”ŸæˆWiki Markdown
    return 0 if generate_wiki_md(stats, args.output_md) else 1

if __name__ == '__main__':
    exit(main())